# Detector-augmented (RAM taps) DQN experiment for Breakout
experiment:
  name: breakout_objenc_ram_dqn
  seed: 321
  total_timesteps: 3000000
  eval_interval: 100000
  eval_episodes: 5
  save_interval: 500000
  rollout_video: true
  video_length: 2000
  notes: "WIP: Dueling Double-DQN with RAM-tap detector features concatenated to visual encoder"

environment:
  id: ALE/Breakout-v5
  frame_skip: 4
  frame_stack: 4
  noop_max: 30
  terminate_on_life_loss: true
  render_mode: null

# Detector configuration will be consumed by upcoming wrappers/utilities
# TODO: implement detector pipeline in src/simple_game/detectors.py
# and integrate with training build_env logic.
detector:
  type: ram_tap
  features:
    - paddle_x
    - ball_x
    - ball_y
    - ball_vx
    - ball_vy
    - brick_bitmap
  normalization: builtin  # TODO: define normalization scheme
  output_mode: concat

model:
  algo: dqn
  policy: src.simple_game.policies.DetectorAugmentedDuelingCnnPolicy
  n_envs: 1
  vec_env_class: dummy
  learning_rate: 0.0001
  buffer_size: 100000
  learning_starts: 50000
  batch_size: 32
  tau: 1.0
  gamma: 0.99
  train_freq: 4
  gradient_steps: 1
  target_update_interval: 10000
  exploration_fraction: 0.1
  exploration_final_eps: 0.01
  max_grad_norm: 10.0
  policy_kwargs:
    net_arch: [512]
    features_extractor_kwargs:
      cnn_output_dim: 256
      detector_hidden_dim: 128
      detector_output_dim: 64
  prioritize_replay: true
  prioritized_replay_alpha: 0.6
  prioritized_replay_beta0: 0.4
  prioritized_replay_beta_iters: null
  prioritized_replay_eps: 0.000001

logging:
  tensorboard_log: runs/tensorboard
  checkpoint_dir: runs/checkpoints
  monitor_dir: runs/monitor
  video_dir: runs/videos
  log_interval: 100
  verbose: 1
  save_replay_buffer: true
  enable_profiler: false

hardware:
  device: auto
  num_threads: 4
  mps_fallback: true
